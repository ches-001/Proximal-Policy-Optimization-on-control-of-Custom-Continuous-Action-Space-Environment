{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparison Notebook between a PID controller and the RL (PPO) Policy Network the SinusoidLaneEnv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from networks import ActorNet\n",
    "from custom_env import SinusoidLaneEnv\n",
    "from ppo import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PID Controller Implementation\n",
    "\n",
    "For A PID controller:\n",
    "\n",
    "$u_t = f_p(e_t) + f_i(e_t) + f_d(e_t)$\n",
    "\n",
    "Where:\n",
    "$e_t = y_t - \\hat{y}_t$\n",
    "\n",
    "$f_p(e_t) = k_p \\cdot e_t$\n",
    "\n",
    "$f_d(e_t) = k_i \\cdot  (\\sum_{i=t-h}^h e_i) \\cdot dt$\n",
    "\n",
    "$f_d(e_t) = k_d \\cdot \\frac {e_t - e_{t-1}} {dt}$\n",
    "\n",
    "$u_t$ is the estimated control input at time $t$, $e_t$ is the error at time $t$, $y_t$ is the setpoint at time $t$ and $\\hat{y}_t$ is the output from the estimated control input at time $t$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PID:\n",
    "    def __init__(\n",
    "            self, \n",
    "            kp: float, \n",
    "            ki: float, \n",
    "            kd: float, \n",
    "            dt: float, \n",
    "            integral_horizon: int=10):\n",
    "        \n",
    "        self.kp = kp; self.ki = ki; self.kd = kd; self.dt = dt\n",
    "        self.__past_errors = deque(maxlen=integral_horizon)\n",
    "        self.__last_e = 0\n",
    "\n",
    "    def proportional(self, e: float):\n",
    "        return self.kp * e\n",
    "    \n",
    "    def integral(self, e: float):\n",
    "        self.__past_errors.append(e)\n",
    "        ie = np.sum(self.__past_errors) * self.dt\n",
    "        return self.ki * ie\n",
    "    \n",
    "    def derivative(self, e: float):\n",
    "        de = (e - self.__last_e) / self.dt\n",
    "        self.__last_e = e\n",
    "        return self.kd * de\n",
    "    \n",
    "    def pid(self, e: float):\n",
    "        p = self.proportional(e)\n",
    "        i = self.integral(e)\n",
    "        d = self.derivative(e)\n",
    "        u =  p + i + d\n",
    "        return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PID Controller Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID controller reward: 105.55127744306726\n"
     ]
    }
   ],
   "source": [
    "env = SinusoidLaneEnv()\n",
    "kp = 2; ki = 1; kd = 1; dt = env.dx; u = np.random.randn(1); done = False\n",
    "\n",
    "state, info = env.reset()\n",
    "controller = PID(kp, ki, kd, dt, integral_horizon=20)\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    state, reward, terminate, truncate, info = env.step(u)\n",
    "    env.render()\n",
    "    done = terminate or truncate\n",
    "    total_reward += reward\n",
    "    setpoint = info[\"setpoint\"]\n",
    "    e = (setpoint - state).mean()\n",
    "    u = controller.pid(e)\n",
    "    u = np.array([u])\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"PID controller reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RL (PPO) Policy Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL (PPO) controller reward: 121.19168768178076\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "train_env = SinusoidLaneEnv()\n",
    "obs_space = train_env.observation_space._shape[0]\n",
    "action_space = train_env.action_space._shape[0]\n",
    "\n",
    "policy_agent = ActorNet(obs_space, action_space, hidden_size)\n",
    "policy_agent.load_state_dict(torch.load(os.path.join(f\"params/policy.pth.tar\"), map_location=\"cpu\"))\n",
    "policy_agent.eval()\n",
    "\n",
    "test_env = SinusoidLaneEnv()\n",
    "total_reward, _ = evaluate(test_env, policy_agent, render_env=True, close_env=True)\n",
    "print(f\"RL (PPO) policy reward: {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
